---
title: "Feature Engineering"
output: html_notebook
---

```{r, include=FALSE}
libs <- c("tidyverse", "reshape2", "patchwork", "statmod")
lapply(libs, function(lib) {
  if(! lib %in% installed.packages()) install.packages(lib)
})

library(dplyr)
library(ggplot2)
options(scipen = 5)
```

Load the data from Lesson 5 where we prepared Claims and Policy information in one dataset. We will also convert the Time Exposure from a date object to the integer amount of days.

```{r}
dt_pol_w_claims <- readRDS("./Data/lesson5_dt_pol_w_claims_full.rds") %>% 
  mutate(Time_Exposure = as.integer(Time_Exposure))
```

Let's also define the potential predictors and their types - it is important to differentiate between categorical and numeric variables. 

```{r}
var_types <- c(
  Year="category", Time_on_book="number", Customer_Type="category",
  D_age="number", D_age_banded="category", BonusMalus="number",
  D_ZIP="category", VEH_type="category", Veh_type1="category", 
  Veh_type2="category", VEH_brand="category", Construct_year="number",
  Capacity="number", Nr_of_seats="category", Sum_insured="number")

all_vars <- names(var_types)

for(i in seq_along(var_types)) {
  var_nm <- names(var_types)[[i]]
  var_type <- var_types[[i]]
  conversion_fun <- switch(var_type, category=as.character, number=as.numeric)
  
  dt_pol_w_claims <- dt_pol_w_claims %>% 
    mutate_at(var_nm, conversion_fun)
}
```

Let's remove any observations that contain missing values for some of our predictors. In other words, let's keep only the complete cases (rows).

```{r}
dt_pol_w_claims <- dt_pol_w_claims %>% 
  filter(complete.cases(.[all_vars]))
```

Let's repeat the setup for model validation methodology from the previous exercise and define an 'rmse' function.

```{r}
set.seed(58742) # to fix randomizer

n_rows <- nrow(dt_pol_w_claims)
train_idx <- sample(n_rows, round(0.80*n_rows)) # 80/20 split between train and valid

train <- dt_pol_w_claims[train_idx, ]
valid <- dt_pol_w_claims[-train_idx, ]

rmse <- function(prediction, actual, n_groups=5) {
  if(n_groups>0) {
    prediction_order <- order(prediction)
    prediction <- prediction[prediction_order]
    actual <- actual[prediction_order]
    
    prediction_group <- cut(seq_along(actual), n_groups, labels=FALSE)
    actual_group <- cut(seq_along(prediction), n_groups, labels=FALSE)
    prediction_split <- split(prediction, prediction_group)
    actual_split <- split(actual, actual_group)
    
    prediction <- sapply(prediction_split, mean)
    actual <- sapply(actual_split, mean)  
  }
  
  se <- (prediction-actual)^2
  mse <- mean(se)
  sqrt(mse)
}
```

### Finalizing model structure

Great! Now we have set up the methodology for checking the performance of the model so we are completely ready to start improving it.

This was the very basic glm model we fit in the last lesson. This time, let's fit it only on the training data to use the correct methodology.

```{r}
train_nonzero <- train %>% 
  filter(Burning_Cost > 0)

valid_nonzero <- valid %>% 
  filter(Burning_Cost > 0)

model0 <- glm(
  data = train_nonzero,
  formula = Burning_Cost ~ Veh_type2,
  family = Gamma(link="log")
)

summary(model0)

# broom package can be used for tidy model summary
broom::tidy(model0)
broom::glance(model0)
broom::augment(model0)
```

Let's find out how risky are trucks compared to normal cars (base category included in intercept). We can achieve this by taking the model coefficients and applying the inverse link function.

The model uses log link function, therefore the inverse link function is the natural exponentiation.

```{r}
exp(summary(model0)$coef[,1])
```

We talked bout the fact that there can be multiple options for family (distribution) and link/inverse link functions for GLM models. While the default link function used with Gamma is the inverse function, in insurance context, it is more helpful to use the log link function (and exponentiation as inverse link). The interpretation and implementation is easier.

However, the Gamma distribution is constrained only to positive (non-zero) values, which can create a little bit biased result and would need to be later on adjusted based on claim frequency model using Poisson distribution.

Instead of needing to create two models, let's use assume our target has Tweedie distribution, which is a composite of a Poisson and Gamma distributions that can accommodate also zero values.

Finally, let's also weigh our observations according to the exposure. The target variable is already standardized by the time exposure, but if we compare two observations with the same value of target (e.g. 50$ burning cost per 1day of exposure) the more credible information should come from an observation with more days of exposure, right?

```{r}
model1 <- glm(data = train,
              formula = Burning_Cost ~ Veh_type2,
              family = statmod::tweedie(var.power=1.5, link.power=0),
              weights = Time_Exposure)

summary(model1)

print("Training RMSE:")
rmse(predict(model1, train, type="response"), train$Burning_Cost)
```

Let's find out how risky are various vehicle types compared to normal cars (base category included in intercept). As we can see, it's very easy with the log link.

```{r}
exp(summary(model1)$coef[,1])
```

We now know the basic structure model attributes (distribution, link, weight) and we will proceed to add more variables to the model. Let's review the model building and feature engineering actions we can take.

List of model building actions:
1.  Add a new variable
    - If you bring more information in a model, it is becoming much more accurate as it is able to catch many more various situations
2. Remove some variable
    - Although adding a new variable brings new information to the model, it brings also the noise. You need to find a balance between this. Model should be robust and not overfit.

List of some feature engineering approaches:
1.  Capping strategy to keep only trending part of the variable
2.  Grouping of Categorical variable 
3.  Outlier resolution
4.  Variable transformation (polynomial, log, exp, ...)
5.  Creating interactions

### Add a new variable

As we can see, there are many variables to choose from.

```{r}
glimpse(train[, all_vars])
```

We can use a helper function that is prepared in the `Support` folder. It can help us to:
  - Assess how our fitted variables perform
  - Assess which unfitted variables might provide extra information

```{r}
source("./Support/emb_chart.R") # one-time
```

Let's look at our fitted variable first. 

One line should represent the actual mean - Time_Exposure weighted mean of Burning_Cost over different vehicle types.
The other line should represented the fitted (predicted) mean - Time_Exposure weighted mean of Burning_Cost model predictions over different vehicle types.

We can't even see the second line, since they're identical. This happens only when there's a single variable in the model. In this case the model simply replicates the weighted mean from the data. But the lines will start diverging slightly, as we keep adding variables to the model.

```{r}
this_model <- model1
fitted_vars <- strsplit(as.character(this_model$formula)[-c(1,2)], " \\+ ")[[1]]

for(var_nm in fitted_vars) {
  emblem_graph(
    model = this_model,
    x_var =  var_nm
  ) %>% 
    show()
}
```

Let's take a look at the (many) unfitted variables. The lines should be pretty diverging, especially if an information from a certain variable is missing in the model. That means the model doesn't yet have such information to be able to differentiate between high and low risks within the variable in question.

Adding such variable to the model might help to improve its performance.

```{r}
this_model <- model1
unfitted_vars <- setdiff(all_vars, as.character(this_model$formula)[-c(1,2)])

for(var_nm in unfitted_vars) {
  emblem_graph(
    model = this_model,
    x_var =  var_nm
  ) %>% 
    show()
}
```

Let's say we decide to add `Construct_year` variable, because the difference between actual mean and fitted mean are pretty significant on both sides of the exposure distribution. 

```{r}
model2 <- glm(data = train,
              formula = Burning_Cost ~ Veh_type2 + Construct_year,
              family = statmod::tweedie(var.power=1.5, link.power=0),
              weights = Time_Exposure)

summary(model2)

print("Training RMSE:")
rmse(predict(model2, train, type="response"), train$Burning_Cost)
```

Let's review the fitted variables visually. We can see that the first variable is fit almost perfectly - the lines are aligned. This is caused mainly by two things:
1) There are very few variables in the model
2) Veh_type2 variable is a categorical one, meaning there's one coefficient (dummy variable) per category, which improves the fit

On the other hand Construct_year doesn't seem to be fit very well, the two lines are quite different from each other.

```{r}
this_model <- model2
fitted_vars <- strsplit(as.character(this_model$formula)[-c(1,2)], " \\+ ")[[1]]

for(var_nm in fitted_vars) {
  emblem_graph(
    model = this_model,
    x_var =  var_nm
  ) %>% 
    show()
}
```

### Merging, capping and other feature engineering

The visuals above should give us a hint about some of the feature engineering operations that we can attempt. Let's start with merging, which is a feature engineering adjustment that can be done with categorical types of variables.

With merging, we usually merge categories based on the following rules of thumb:
- Categories with low exposure (weight) can be merged with the base category (most frequently ocurring)
- Categories with zero target values can be merged with the base category
- Categories with enough exposure but very similar coefficient / multiplier can be merged together

Let's try to do the merging operation with the `Veh_type2` variable and refit the model.

Make sure to create a new variable (not rewrite the original) and do the operation in both training and validation dataset. This is important for later model evaluation function to work properly.

```{r}
train <- train %>% 
  mutate(Veh_type2_merged = ifelse(Veh_type2 %in% c("", "OTHER", "TRACTOR"), "CAR", Veh_type2))

valid <- valid %>% 
  mutate(Veh_type2_merged = ifelse(Veh_type2 %in% c("", "OTHER", "TRACTOR"), "CAR", Veh_type2))

model3 <- glm(data = train,
              formula = Burning_Cost ~ Veh_type2_merged + Construct_year,
              family = statmod::tweedie(var.power=1.5, link.power=0),
              weights = Time_Exposure)

summary(model3)

print("Training RMSE:")
rmse(predict(model3, train, type="response"), train$Burning_Cost)
```

Let's take a look at the adjustment

```{r}
this_model <- model3
fitted_vars <- strsplit(as.character(this_model$formula)[-c(1,2)], " \\+ ")[[1]]

for(var_nm in fitted_vars) {
  emblem_graph(
    model = this_model,
    x_var =  var_nm
  ) %>% 
    show()
}
```

As a next step, we can try capping the `Construct_year` variable. This means setting a cap (floor or ceiling) on the minimum, maximum (or both) value.

For example, we'll limit the `Construct_year` between 2006 and 2015, and we'll set the higher or lower values to be equal to the bounds of our selected interval.

Make sure to create a new variable (not rewrite the original) and do the operation in both training and validation dataset. This is important for later model evaluation function to work properly.

```{r}
train <- train %>% 
  mutate(
    Construct_year_capped = Construct_year,
    Construct_year_capped = ifelse(Construct_year_capped < 2005, 2005, Construct_year_capped),
    Construct_year_capped = ifelse(Construct_year_capped > 2015, 2015, Construct_year_capped)
  )

valid <- valid %>% 
  mutate(
    Construct_year_capped = Construct_year,
    Construct_year_capped = ifelse(Construct_year_capped < 2005, 2005, Construct_year_capped),
    Construct_year_capped = ifelse(Construct_year_capped > 2015, 2015, Construct_year_capped)
  )

model4 <- glm(data = train,
              formula = Burning_Cost ~ Veh_type2_merged + Construct_year_capped,
              family = statmod::tweedie(var.power=1.5, link.power=0),
              weights = Time_Exposure)

summary(model4)

print("Training RMSE:")
rmse(predict(model4, train, type="response"), train$Burning_Cost)
```

```{r}
this_model <- model4
fitted_vars <- strsplit(as.character(this_model$formula)[-c(1,2)], " \\+ ")[[1]]

for(var_nm in fitted_vars) {
  emblem_graph(
    model = this_model,
    x_var =  var_nm
  ) %>% 
    show()
}
```

Now we're able to better see whether there's any visible trend in the observed data and whether the model catches such trend. We can conclude that this not the best variable, as there's no reasonable trend, especially for the categories with the main distribution of exposure.

We could for example remove this variable and try a different one, creating a new model.

Finally, there's a third useful group of feature engineering adjustments - changing units and/or rounding. This might be especially useful when dealing with variables such as `D_zip`, `Capacity` or `VEH_brand`.

We could also theoretically consider variable transformations, such as polynomials, square roots, logs or interactions, but these often make the model more complex and difficult to understand (especially for business partners). These are unfortunately out of scope of this lesson.

Kratky dotaznik spokojnosti s predmetom
https://forms.office.com/e/HffTAtKk03

---
title: "Target Analysis"
output: html_notebook
---

Great, now you wrote about your ideas about the target, let's think about it together.

- _What is the event, object or idea you would like to predict?_

    We definitely want to find out the risk of the specific client and predict it for potential clients.
   
- _Is it measurable?_

    How can we measure risk? Which information identify risky client? 
   
    This is something we need to discuss more and have whole section about it. Example of a good risk indicator can be Ultimate Losses, Ultimate Loss Ratio, Ultimate Frequency and it depends pretty much on what kind of risk you are interested in. 
   
     Let's take an example. If you are really interested in risk described in dollar values, you will probably end up using Ultimate Loss as good measurable variable indicating riskiness of the client.
   
    But, some of the insurance coverages pay only fixed amount of dollars when claims occur. In such case there is not so much importance on dollar value paid, but much more important thing is how many claims a client had. We usually choose Ultimate Frequency as a good target or combination of Loss amount and Frequency unit.
   
    So the answer here depends pretty much on the definition of the insurance product and its specific risk you are trying to predict.

So once again...

> What is a good measure of a risky client?

Hmm. `Paid` losses might be a good start.

```{r}
dt_pol_w_claims <- dt_Policy %>% 
  left_join(dt_Claims, by = c("NrPolicy", "NrObject"))

# TODO : Get Top 5 highest values of Paid 
dt_pol_w_claims %>% 
    filter(!is.na(Paid)) %>% 
    select(NrPolicy, NrObject, NrClaim, Paid) %>% 
    arrange(desc(Paid)) %>% 
    head(5)
```

### Exposure
> Is it enough? Can we say, those first three client have similar risk?

Well...it might be not enough. Let's look at the same example, but we will add some more information. 
Specifically, we will show when the policy started and when it was ended.

```{r}
# TODO : Get Top 5 highest values of Paid and their Dt_Exp_Start and Dt_Exp_End 
dt_pol_w_claims %>% 
    filter(!is.na(Paid)) %>% 
    select(NrPolicy, NrObject, Paid, Dt_Exp_Start, Dt_Exp_End) %>% 
    arrange(desc(Paid)) %>% 
    head(5)

```


> What's different among those first three clients?

The third client asked for insurance cover only for three months! And during those three months they have similar loss amount as other clients have during one year. This leads to redefining the risk of the third client to be 4-times(!) riskier than first two clients from the table above.

What we described here is term `exposure`. The exposure can be different for different portfolios we analyse. E.g. it can be square root of sum insured for property business or mileage for trucks insurance. We often talk about exposure as unit of insurance cover. There are many definitions. 

So let's create our exposure based on time a client was covered.
```{r}
# tip: lubridate is great for manipulation of dates
library(lubridate)

dt_pol_w_claims <- dt_pol_w_claims %>% 
    mutate(Time_Exposure = lubridate::dmy(Dt_Exp_End) - lubridate::dmy(Dt_Exp_Start) + 1)

dt_pol_w_claims %>%
  filter(!is.na(Paid)) %>% 
  arrange(desc(Paid)) %>% 
  dplyr::select(Dt_Exp_Start, Dt_Exp_End, Time_Exposure) %>% 
  head(5)
```

Did you realize for some years there is 365 and for some 366 days? `lubridate` knows which year is a leap year. Cool, right?

#### Ultimate Losses and Burning Cost
Ultimate Losses are something we end up with after paying for the individual claims. They can include various parts of the claim e.g. (Losses, Reserves, Inflation, Expenses to arrange the claim, ...).

Burning Cost we call overall cost per some metric of exposure, in our case we are talking about _dollar loss per day insured_.

```{r}
# TODO : Compute Burning_Cost as Ult_Loss per day
# Hint: use as.integer(Time_Exposure) to get the number of days
dt_pol_w_claims <- dt_pol_w_claims %>% 
    mutate(
        Ult_Loss = Paid + Reserves,
        Burning_Cost = ifelse(is.na(Ult_Loss), 0, Ult_Loss / as.integer(Time_Exposure))
    )

dt_pol_w_claims %>% 
    filter(Burning_Cost != 0) %>% 
    select(NrClaim, Ult_Loss, Time_Exposure, Burning_Cost) %>% 
    head()

dt_pol_w_claims %>% 
    filter(Burning_Cost == 0) %>% 
    select(NrClaim, Ult_Loss, Time_Exposure, Burning_Cost) %>% 
    head()
```

Let's continue with the questions which help us to identify a good target.

- _Let's say you finished your model, how did the prediction help you to solve your insurance issue?_

    When we have a good model, which could predict Ultimate Time weighted Loss accuratelly, it will help us to build suitable price we should offer to a potential client. The price should ensure it is not too high for less risky client, so we are competitive on the market, but also high enough to cover for potential claims. Btw we might call such price _Technical Price_.

- _Are there any potential independent variables, they might have relationship to the target you propose?_

    This is something we can solve using _One-Way Analysis_

## One-Way Analysis
Perfect! It looks like we have found a good target, which might be a good measure for risky clients.

Now, let's try to think about the __reasons__ of client being more risky than others.

#### Exercise
Do you have any idea which type of client is definitely riskier than other? 
Write a few sentences to your notes and commit.


--------------------------------------------------------------------------------

One-Way analysis means we always look for one explanatory variable and one which we try to explain, in our case it's our target we identified. So first of all it makes sense to look into them as basic _scatterplot_.

For the first one-way analysis we will try to explore feature about vehicle type of client: `Veh_type2`

```{r}
#install.packages("ggplot2")
library(ggplot2)

dt_pol_w_claims %>% ggplot(
    aes(x = Veh_type2, y = Burning_Cost)
) + geom_jitter()

```


Does it help you to identify any trend? Hmm...looks like outliers messing it up. Let's go for numbers then.

```{r}
# TODO: Compute mean() and median() of the Burning_Cost
dt_pol_w_claims %>% 
    group_by(Veh_type2) %>% 
    summarize(
        BC_avg = mean(Burning_Cost),
        BC_med = median(Burning_Cost),
        cnt = n()
    ) %>% arrange(
        desc(BC_avg)
    )
```

Why did we choose those three metrics? Do you see any story behind them? Let's try to visualize the averages.

```{r}
dt_pol_w_claims %>% 
  group_by(Veh_type2) %>% 
  summarize(
      BC_avg = mean(Burning_Cost),
      BC_med = median(Burning_Cost),
      cnt = n()
  ) %>%
  filter(BC_avg>0) %>%
  ggplot(aes(y = BC_avg, x = Veh_type2)) +
  geom_col()
```

Let's try to remove the outliers and take a look at our data again.

```{r}
dt_pol_w_claims %>% 
  filter(Burning_Cost <= 100) %>%
  group_by(Veh_type2) %>% 
  summarize(
      BC_avg = mean(Burning_Cost),
      BC_med = median(Burning_Cost),
      cnt = n()
  ) %>%
  filter(BC_avg>0) %>% 
  ggplot(aes(y = BC_avg, x = Veh_type2)) +
  geom_col()
```


Two things happened here:

    - outliers mess it up so much, that this feature is definitelly not good predictor alone, we are not able to explain those outliers using only one feature describing vehicle type.
    - but...there is definitelly some trend, which might be usefull in next stages of modelling. (saying that there is definitelly some trend might be too strong and we should use also other methods to confirm this. e.g. ANOVA)

#### Exercise
Choose another feature and try to find out the story behind data, using similar approach as above.
